{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eacb201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:23:34.876519Z",
     "iopub.status.busy": "2024-09-14T15:23:34.876135Z",
     "iopub.status.idle": "2024-09-14T15:23:35.605416Z",
     "shell.execute_reply": "2024-09-14T15:23:35.604591Z",
     "shell.execute_reply.started": "2024-09-14T15:23:34.876494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 15 18:11:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   45C    P0              26W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3587b03-4439-4dd1-bc9a-caad0bc940b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:38:28.560251Z",
     "iopub.status.busy": "2024-09-14T15:38:28.559882Z",
     "iopub.status.idle": "2024-09-14T15:38:28.563832Z",
     "shell.execute_reply": "2024-09-14T15:38:28.562987Z",
     "shell.execute_reply.started": "2024-09-14T15:38:28.560222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398aba8e-8ce5-4a75-9978-6153ceca60f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:29:32.154835Z",
     "iopub.status.busy": "2024-09-14T15:29:32.154453Z",
     "iopub.status.idle": "2024-09-14T15:29:32.762711Z",
     "shell.execute_reply": "2024-09-14T15:29:32.762042Z",
     "shell.execute_reply.started": "2024-09-14T15:29:32.154797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         100G   44G   57G  44% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\r\n",
      "/dev/nvme0n1p1  100G   44G   57G  44% /run\r\n",
      "tmpfs            14G     0   14G   0% /dev/shm\r\n",
      "/dev/nvme2n1    2.0G   35M  1.9G   2% /home/jovyan\r\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\r\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\r\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\r\n",
      "tmpfs           7.7G  3.0M  7.7G   1% /run/nvidia-persistenced/socket\r\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "# Space I have \n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9aae0a-d8ab-445e-bfe1-e4e0587396c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T16:10:04.558518Z",
     "iopub.status.busy": "2024-09-14T16:10:04.558149Z",
     "iopub.status.idle": "2024-09-14T16:10:04.565025Z",
     "shell.execute_reply": "2024-09-14T16:10:04.564245Z",
     "shell.execute_reply.started": "2024-09-14T16:10:04.558493Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking all the environment variables available\n",
    "#os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af23b184-a6f0-4304-ae2a-60ddd8028b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T16:13:14.825052Z",
     "iopub.status.busy": "2024-09-14T16:13:14.824668Z",
     "iopub.status.idle": "2024-09-14T16:13:14.828486Z",
     "shell.execute_reply": "2024-09-14T16:13:14.827834Z",
     "shell.execute_reply.started": "2024-09-14T16:13:14.825027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As hugging face stores the file inside the cache folder inside HOME directory which in our case doesn't have mmuch storage, so changing the default storage of Hugging Face\n",
    "# HF_HOME is the variable which contains the location to store the Hugging Face data\n",
    "# MAKE SURE TO RESTART THE KERNEL AFTER THIS\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa8c8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/run/cache/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcfff1-2b2d-41b1-809d-51e6e10f85e3",
   "metadata": {},
   "source": [
    "# Copied from Hugging Face <a href=\"https://huggingface.co/docs/transformers/en/model_doc/mistral\"> Webage of the model</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325d3d6",
   "metadata": {},
   "source": [
    "## NOTE: <span style=\"color:red\"> To access the mistral model, first log in into the hugging space and then accept the agreement <a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\">here</a>, and then login into Hugging Face from this notebook</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3e6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /run/cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Logging into Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(os.environ[\"HUGGING_FACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143fbc0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Requirements as mentioned in the documentation to run this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7778035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers.git\n",
    "#!pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec33503",
   "metadata": {},
   "source": [
    "## Loading of Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b5e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16dc0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a575c816e9184e379531b466b235fb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910dc0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:27:47.459458Z",
     "iopub.status.busy": "2024-09-14T15:27:47.459075Z",
     "iopub.status.idle": "2024-09-14T15:27:55.273779Z",
     "shell.execute_reply": "2024-09-14T15:27:55.273072Z",
     "shell.execute_reply.started": "2024-09-14T15:27:47.459427Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> My favourite condiment is ketchup. There’s no arguing with me on this matter. And, as you know, there are ketchup lovers and ketchup haters in every family. Not mine! I have been successful so far in training everyone, even my husband, to love ketchup, except for my youngest daughter, and I’m not sure there’s any hope yet for her.\\n\\nI’m happy to eat ketchup on just about anything. On fried'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bc242",
   "metadata": {},
   "source": [
    "## When using Pipeline from Transformers and creating a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097a404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48396611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:599: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:604: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mayonnaise. I love it. I love it on sandwiches, I love it on burgers, I love it on fries, I love it on chicken, I love it on fish, I love it on pizza, I love it on salads, I love it on hot dogs, I love it on eggs, I love it on toast, I love it on pasta, I love it on rice, I love it on potatoes, I love it on vegetables, I love it on fruit, I love it on meat, I love it on cheese, I love it on bread, I love it on crackers, I love it on chips, I love it on pancakes, I love it on waffles, I love it on ice cream, I love it on cake, I love it on cookies, I love it on muffins, I love it on scones, I love it on muffins, I love it on pies, I love it on tarts, I love it on cakes, I love it on pastries, I love it on doughnuts, I love it on bagels, I love it on croissants, I love it on brioche, I love it on buns, I love it on rolls, I love it on breadsticks, I love it on baguettes, I love it on focaccia, I love it on ciabatta, I love it on pita, I love it on naan, I love it on tortillas, I love it on wraps, I love it on pancakes, I love it on waffles, I love it on crepes, I love it on French toast, I love it on omelettes, I love it on scrambled eggs, I love it on fried eggs, I love it on poached eggs, I love it on boiled eggs, I love it on eggs Benedict, I love it on eggs Florentine, I love it on eggs Royale, I love it on eggs in purgatory, I love it on eggs in hell, I love it on eggs in purgatory, I love it on eggs in hell, I love it on eggs in purgatory, I love it on eggs in hell, I love it on eggs in purgatory, I love it on\n"
     ]
    }
   ],
   "source": [
    "response = generator(prompt, max_length=500, temperature=0.7, top_p=0.95, num_return_sequences=1)\n",
    "response_final = response[0]['generated_text']\n",
    "print(response_final[len(prompt):].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d862e",
   "metadata": {},
   "source": [
    "# ===== <span style=\"color:blue\">Combining everything together to form a RAG</span> ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f382217",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">NOTE: Be aware that the model, tokenizer, generator used in this combined part has been defined previously above</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a28f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-15 18:13:50--  https://raw.githubusercontent.com/vaibhavcodes/LLM_RAG/main/Part_01_Basics/similarDocSearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3320 (3.2K) [text/plain]\n",
      "Saving to: ‘similarDocSearch.py’\n",
      "\n",
      "similarDocSearch.py 100%[===================>]   3.24K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-15 18:13:50 (46.6 MB/s) - ‘similarDocSearch.py’ saved [3320/3320]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the Similar Doc Search python file created in Part_01_Basics\n",
    "!rm -f similarDocSearch.py\n",
    "!wget https://raw.githubusercontent.com/vaibhavcodes/LLM_RAG/main/Part_01_Basics/similarDocSearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c7972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarDocSearch import *\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3394a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents fetching from the github\n",
    "docs_url = \"https://github.com/vaibhavcodes/LLM_RAG/blob/main/Part_01_Basics/data/documents.json?raw=1\"\n",
    "docs_response = requests.get(docs_url)\n",
    "docs_raw = docs_response.json()\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course'] \n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8b56db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<similarDocSearch.similarDocSearch at 0x7f5982fe9850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the class similarDocSearch\n",
    "obj = similarDocSearch(text_fields = [\"question\", \"text\", \"section\"])\n",
    "# Fitting\n",
    "obj.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc23080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would search the most similar documents to our query and will return it\n",
    "def search(query):\n",
    "    boost = {'question': 3, \n",
    "         'section': 0.5} # Used to give a field more or less importance \n",
    "    \n",
    "    filter_course = \"data-engineering-zoomcamp\"\n",
    "    num_results = 5\n",
    "    \n",
    "    results = obj.search(query = query, boosts=boost, filter_course=filter_course, num_results=num_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5955775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would build up the prompt\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. \n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    If the CONTEXT doesn't contains the answer, output None\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \n",
    "    Based on the QUESTION and CONTEXT provide, please provide the ANSWER in only 200 words.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Context creation to be passed into the prompt\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        #context = context + f\"section: {doc['section']}\\n question: {doc['question']}\\n answer: {doc['text']}\\n \\n\"\n",
    "        context = context + f\"{doc['question']}\\n {doc['text']}\\n \\n\"\n",
    "    \n",
    "    # Prompt formatting\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    \n",
    "    return(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "372ddb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would use LLM\n",
    "def llm(prompt):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "    result = tokenizer.batch_decode(generated_ids)[0]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1dfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would make a whole RAG\n",
    "def RAG(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ba9fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. \n",
      "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "    If the CONTEXT doesn't contains the answer, output None\n",
      "    \n",
      "    QUESTION: I just discovered the course. Can i still join it??\n",
      "    \n",
      "    CONTEXT: \n",
      "    Course - Can I still join the course after the start date?\n",
      " Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      " \n",
      "Course - When will the course start?\n",
      " The purpose of this document is to capture frequently asked\n",
      "technical question\n",
      "The next cohort starts in Jan 2025. More inFo at DTC Article.\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      " \n",
      "Course - What can I do before the course starts?\n",
      " You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      " \n",
      "Course - Which playlist on YouTube should I refer to?\n",
      " All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\n",
      "Below is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\n",
      " \n",
      "How can we contribute to the course?\n",
      " Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve the text or the structure of the repository.\n",
      " \n",
      "\n",
      "    \n",
      "    Based on the QUESTION and CONTEXT provide, please provide the ANSWER in only 200 words.\n",
      "    You're only allowed to use the context given above.\n",
      "    \n",
      "    I've noticed there are answers written out on the internet. PLEASE do not copy paste\n",
      "    the existing answers or you will be disqualified.\n",
      "    \n",
      "    I'm looking for originality, thought process in creating the answer, and it should be a logical\n",
      "    extension of the context you're given. As a TA, you are required to be creative. If not\n"
     ]
    }
   ],
   "source": [
    "q = \"I just discovered the course. Can i still join it??\"\n",
    "print(RAG(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee2e84",
   "metadata": {},
   "source": [
    "#### The result obtained is not good, but using pipeline improves the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159645a",
   "metadata": {},
   "source": [
    "## When using Pipeline from transformers- generator used here has been defined previously before combining all the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb04586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would use LLM\n",
    "# generator defined before combining the code\n",
    "def llm(prompt):\n",
    "    response = generator(prompt, max_new_tokens=100, max_length=100, temperature=0.7, top_p=0.95, num_return_sequences=1)\n",
    "    response_final = response[0]['generated_text']\n",
    "    # As can be seen from previous output that model also gives the prompt and at the end gives the answer\n",
    "    # So we are ignoring the prompt text via slicing\n",
    "    result  = response_final[len(prompt):].strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2708311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would make a whole RAG\n",
    "def RAG(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11aea21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:599: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:604: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "    Answer:\n",
      "    Yes, you can join the course after the start date.\n",
      "    The course is open to new students until the end of the semester.\n",
      "    After that, you can still join the course, but you will not be able to participate in the final exam.\n",
      "    You will have to take the final exam at the end of the semester.\n",
      "    You can join the course at any time during the semester.\n",
      "    You can join the\n"
     ]
    }
   ],
   "source": [
    "q = \"I just discovered the course. Can i still join it??\"\n",
    "print(RAG(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dcdab",
   "metadata": {},
   "source": [
    "# <span style=\"colore:green\">To improve the output, play around with prompt and tuning the parameters</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d9073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
