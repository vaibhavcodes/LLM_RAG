{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eacb201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:23:34.876519Z",
     "iopub.status.busy": "2024-09-14T15:23:34.876135Z",
     "iopub.status.idle": "2024-09-14T15:23:35.605416Z",
     "shell.execute_reply": "2024-09-14T15:23:35.604591Z",
     "shell.execute_reply.started": "2024-09-14T15:23:34.876494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 14 16:39:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   31C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3587b03-4439-4dd1-bc9a-caad0bc940b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:38:28.560251Z",
     "iopub.status.busy": "2024-09-14T15:38:28.559882Z",
     "iopub.status.idle": "2024-09-14T15:38:28.563832Z",
     "shell.execute_reply": "2024-09-14T15:38:28.562987Z",
     "shell.execute_reply.started": "2024-09-14T15:38:28.560222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398aba8e-8ce5-4a75-9978-6153ceca60f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:29:32.154835Z",
     "iopub.status.busy": "2024-09-14T15:29:32.154453Z",
     "iopub.status.idle": "2024-09-14T15:29:32.762711Z",
     "shell.execute_reply": "2024-09-14T15:29:32.762042Z",
     "shell.execute_reply.started": "2024-09-14T15:29:32.154797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         100G   30G   71G  30% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\r\n",
      "/dev/nvme0n1p1  100G   30G   71G  30% /run\r\n",
      "tmpfs            14G     0   14G   0% /dev/shm\r\n",
      "/dev/nvme2n1    2.0G  3.3M  1.9G   1% /home/jovyan\r\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\r\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\r\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\r\n",
      "tmpfs           7.7G  3.0M  7.7G   1% /run/nvidia-persistenced/socket\r\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\r\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "# Space I have \n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9aae0a-d8ab-445e-bfe1-e4e0587396c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T16:10:04.558518Z",
     "iopub.status.busy": "2024-09-14T16:10:04.558149Z",
     "iopub.status.idle": "2024-09-14T16:10:04.565025Z",
     "shell.execute_reply": "2024-09-14T16:10:04.564245Z",
     "shell.execute_reply.started": "2024-09-14T16:10:04.558493Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking all the environment variables available\n",
    "#os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af23b184-a6f0-4304-ae2a-60ddd8028b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T16:13:14.825052Z",
     "iopub.status.busy": "2024-09-14T16:13:14.824668Z",
     "iopub.status.idle": "2024-09-14T16:13:14.828486Z",
     "shell.execute_reply": "2024-09-14T16:13:14.827834Z",
     "shell.execute_reply.started": "2024-09-14T16:13:14.825027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As hugging face stores the file inside the cache folder inside HOME directory which in our case doesn't have mmuch storage, so changing the default storage of Hugging Face\n",
    "# HF_HOME is the variable which contains the location to store the Hugging Face data\n",
    "# MAKE SURE TO RESTART THE KERNEL AFTER THIS\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa8c8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/run/cache/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcfff1-2b2d-41b1-809d-51e6e10f85e3",
   "metadata": {},
   "source": [
    "# Copied from Hugging Face <a href=\"https://huggingface.co/google/flan-t5-xl\"> Webage of the model</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910dc0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T15:27:47.459458Z",
     "iopub.status.busy": "2024-09-14T15:27:47.459075Z",
     "iopub.status.idle": "2024-09-14T15:27:55.273779Z",
     "shell.execute_reply": "2024-09-14T15:27:55.273072Z",
     "shell.execute_reply.started": "2024-09-14T15:27:47.459427Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-14 16:41:59,647] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f8ebe7-4401-4f22-b6c1-afb93e3a39f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d8152d00a1441db5c6247f51149a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265320f4dbc42f7ba2be428750cb09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bac0ed52a240eeaf7312b2db85475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf4d6a0e6474df79c27af9aa263dd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b8e7b42a024bb68dfbacf13fdf870a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c4bef593314304b2eb4415b0485301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a62b312afc24c9298a1246b91136c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ae8cc00e04cbfaa6ebff04a74147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f314b3257e634e7ab29250696bdf71ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df0cd7a97ee40ca90b05f0e63429b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8aa542a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8733,   16,    3,    9, 7142,    6,  125,   19, 8947,   58,    1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Write in a sentence, what is apple?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "106d6dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 2184,   19,    3,    9, 2728,    5,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46497c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Apple is a fruit.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66debf",
   "metadata": {},
   "source": [
    "From above it can be seen that <pad> and </s> is denoting the starting and end of the output having 0 and 1 as the output value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca4150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714d862e",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Combining everything together to form a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a28f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-14 17:15:53--  https://raw.githubusercontent.com/vaibhavcodes/LLM_RAG/main/Part_01_Basics/similarDocSearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3320 (3.2K) [text/plain]\n",
      "Saving to: ‘similarDocSearch.py’\n",
      "\n",
      "similarDocSearch.py 100%[===================>]   3.24K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-14 17:15:53 (78.9 MB/s) - ‘similarDocSearch.py’ saved [3320/3320]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the Similar Doc Search python file created in Part_01_Basics\n",
    "!rm -f similarDocSearch.py\n",
    "!wget https://raw.githubusercontent.com/vaibhavcodes/LLM_RAG/main/Part_01_Basics/similarDocSearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88c7972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarDocSearch import *\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3394a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents fetching from the github\n",
    "docs_url = \"https://github.com/vaibhavcodes/LLM_RAG/blob/main/Part_01_Basics/data/documents.json?raw=1\"\n",
    "docs_response = requests.get(docs_url)\n",
    "docs_raw = docs_response.json()\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course'] \n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb8b56db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<similarDocSearch.similarDocSearch at 0x7f9bb0317340>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the class similarDocSearch\n",
    "obj = similarDocSearch(text_fields = [\"question\", \"text\", \"section\"])\n",
    "# Fitting\n",
    "obj.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddc23080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would search the most similar documents to our query and will return it\n",
    "def search(query):\n",
    "    boost = {'question': 3, \n",
    "         'section': 0.5} # Used to give a field more or less importance \n",
    "    \n",
    "    filter_course = \"data-engineering-zoomcamp\"\n",
    "    num_results = 5\n",
    "    \n",
    "    results = obj.search(query = query, boosts=boost, filter_course=filter_course, num_results=num_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5955775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would build up the prompt\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. \n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    If the CONTEXT doesn't contains the answer, output None\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Context creation to be passed into the prompt\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\n question: {doc['question']}\\n answer: {doc['text']}\\n \\n\"\n",
    "    \n",
    "    # Prompt formatting\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    \n",
    "    return(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "372ddb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would use LLM\n",
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, max_length=100)\n",
    "    return (tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb1dfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would make a whole RAG\n",
    "def RAG(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ba9fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.</s>\n"
     ]
    }
   ],
   "source": [
    "q = \"I just discovered the course. Can i still join it??\"\n",
    "print(RAG(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e6381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea243e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4d7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
